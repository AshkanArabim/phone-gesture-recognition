{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nwmoY5pQ9f64"},"outputs":[],"source":["# following code from: https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/\n","!pip install csiread\n","\n","from google.colab import drive\n","\n","import tensorflow as tf\n","import keras\n","import os\n","import os.path\n","from keras import layers, optimizers, regularizers, models\n","import numpy as np\n","import csiread\n","\n","import re\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","\n","# used for confusion matrix\n","import seaborn as sn\n","import pandas as pd\n","\n","# used for kfold cross validation\n","import sklearn\n","import sklearn.model_selection\n","\n","# for hyperparameter search\n","import itertools\n","import contextlib\n","\n","# for saving the data\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTNVpN2tvt5x"},"outputs":[],"source":["# mount script\n","\n","drive.mount('/Drive')\n","!ln -s '/Drive/MyDrive/google_colab_files_for_CSI' '/content/REU'\n","root_data_path = os.path.join('REU','csi_data_all')\n","REU = os.path.join('REU')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IKI-FXqECJBz"},"outputs":[],"source":["# visualization funcs for easy debugging\n","def subcarLineChart(gph, log = False, save_path = '', plot_title=\"Magnitude of Signal by Channel and Packet Number\"): # log: use log scale or not\n","    packet_index = gph.shape[1]\n","\n","    fig = plt.figure()\n","    ax1 = fig.add_subplot(111)\n","    plt.title(plot_title)\n","    plt.xlabel(\"Packet #\")\n","    plt.ylabel(\"Magnitude\")\n","    ax1.set_yscale(\"log\") if log else None\n","\n","    num_subcarriers = gph.shape[1]\n","    num_samples = gph.shape[0]\n","    for subcar_index in range(num_subcarriers):\n","        one_carrier = np.take(gph, [subcar_index], axis = 1)\n","        ax1.plot(range(num_samples), one_carrier, lw = .25, label = subcar_index)\n","    if save_path != '':\n","        plt.savefig(save_path)\n","    else:\n","        plt.show()\n","    plt.close()\n","\n","def subcarSpectrogram(gph, log = False, subcar_start = 0, subcar_end = -1, save_path=\"\"): # log: use log scale or not\n","    subcar_end = gph.shape[1] if subcar_end == -1 else subcar_end # replace -1 with valid final index\n","    plt.imshow(\n","        gph,\n","        # extent --> left, rigth, bottom, top\n","        extent = (subcar_start, subcar_end, gph.shape[0], 0),\n","        norm = colors.LogNorm() if log else None,\n","        interpolation='none',\n","    )\n","    plt.xlabel('Subcarriers')\n","    plt.ylabel('Packet #')\n","    plt.colorbar(label = 'intensity')\n","    #   plt.show()\n","    if save_path != \"\":\n","        plt.savefig(save_path)\n","    else:\n","        plt.show()\n","    plt.close()\n","\n","# preprocessing if needed\n","\n","# some funcs for preprocessing\n","def subcar_anomaly_removal(subcar, std_threshold):\n","  z_scores = (subcar - np.mean(subcar)) / np.std(subcar)\n","  to_ditch = np.where(np.abs(z_scores) > std_threshold)\n","  # print(to_ditch)\n","  cleaned = np.copy(subcar)\n","  cleaned[to_ditch] = np.median(subcar) # replace with median of subcarrier\n","\n","  return cleaned\n","\n","def df_anomaly_removal(df, std_threshold):\n","  # cleaned_df = df.apply((lambda x: subcar_anomaly_removal(x, std_threshold)), axis = 0)\n","  cleaned_df = np.apply_along_axis(\n","      (lambda x: subcar_anomaly_removal(x, std_threshold)),\n","      axis = 0,\n","      arr=df\n","      )\n","  return cleaned_df\n","\n","## filters ##\n","def mov_avg(subcar, width):\n","  return np.convolve(subcar, np.ones(width), 'valid') / width\n","\n","def gaussian_filter(size, sigma):\n","  # even integers mess up the gaussian filter\n","  if size % 2 == 0:\n","      raise ValueError(\"Size must be an odd integer.\")\n","\n","  x = np.linspace(-(size // 2), size // 2, size)\n","  filter = np.exp(-x**2 / (2 * sigma**2))\n","  filter /= np.sum(filter)  # Normalize the filter\n","  return filter\n","\n","# use this to apply a filter to an array (1d)\n","def apply_filter(array, filter):\n","    if len(array.shape) != 1:\n","        raise ValueError(\"Input array must be 1-dimensional.\")\n","    if len(filter.shape) != 1:\n","        raise ValueError(\"Filter must be 1-dimensional.\")\n","\n","    # Pad the input array\n","    pad_size = len(filter) // 2\n","    padded_array = np.pad(array, pad_size, mode='constant')\n","\n","    # Apply the filter\n","    filtered_array = np.convolve(padded_array, filter, mode='valid')\n","    return filtered_array\n","\n","def rolling_difference_1d(arr, window_size):\n","    result = []\n","    for i in range(len(arr) - window_size + 1):\n","        result.append(np.abs(arr[i + window_size - 1] - arr[i]))\n","    # no padding: we just care abuot the starting index\n","    return np.array(result)\n","\n","def rolling_sum(arr, window_size):\n","    # print(arr.shape)\n","    return np.convolve(arr, np.ones(window_size), 'valid')\n","\n","def ash_trimming(csi, n_roll = 20, return_index = False):\n","    to_trim = csi # will be used later...\n","\n","    csi = np.apply_along_axis(lambda subcar: rolling_difference_1d(subcar, 3), axis = 0, arr = csi)\n","    csi = np.apply_along_axis(lambda packet: sum(rolling_difference_1d(packet, 2)), axis = 1, arr = csi)\n","    csi = rolling_sum(csi, n_roll)\n","    start_index = np.argmax(csi)\n","    end_index = start_index + n_roll\n","\n","    if return_index:\n","        return start_index\n","    else:\n","        # trim the original based on the starting & ending index\n","        return to_trim[start_index : end_index]\n","\n","# background removal using singular value decomposition --> works with any 2d matrix\n","def svd_background_removal(data, num_sv = 1):\n","    u, s, vh = np.linalg.svd(data)\n","    background = u[:, :num_sv] @ np.diag(s[:num_sv]) @ vh[:num_sv, :]\n","    bg_removed = data - background\n","    return bg_removed - np.min(bg_removed) # shift such that the lowest value is 0\n","\n","def simple_background_removal(data):\n","    vertical_sum = np.mean(data, axis = 0)[np.newaxis, :]\n","    return data - vertical_sum # subtract anything that is static in the subcarriers\n","\n","# actual script starts here\n","def clean(csi_matrix, drop = [], log = False, trim = True, remove_bg = False, window_size = 20, filter_size = 2, filter = '', bg_remove_first = False):\n","    csi_matrix = np.delete(csi_matrix, drop, axis = 1)\n","\n","    csi_matrix = np.abs(csi_matrix)\n","    csi_matrix = df_anomaly_removal(csi_matrix, 2)\n","\n","    # apply specified filter\n","    # if nothing specified, does nothing\n","    if filter == 'average':\n","        csi_matrix = np.apply_along_axis((lambda x: mov_avg(x, filter_size)), arr = csi_matrix, axis = 0)\n","    elif filter == 'gaussian':\n","        filter = gaussian_filter(filter_size, 1)\n","        csi_matrix = np.apply_along_axis((lambda x: apply_filter(x, filter)), arr = csi_matrix, axis = 0)\n","    # take log, replace anything below 1 with 0\n","\n","    if bg_remove_first:\n","        if remove_bg:\n","            csi_matrix = svd_background_removal(csi_matrix)\n","\n","        if trim:\n","            csi_matrix = ash_trimming(csi_matrix, n_roll = window_size)\n","    else:\n","        if trim:\n","            csi_matrix = ash_trimming(csi_matrix, n_roll = window_size)\n","\n","        if remove_bg:\n","            csi_matrix = svd_background_removal(csi_matrix)\n","\n","    if log:\n","        csi_matrix = np.where(csi_matrix <= 1, 0, np.log(csi_matrix))\n","\n","    return csi_matrix\n","\n","def process_dataset_folder(path, trimmed = False, version = 3, create_graphs = False):\n","    # DEBUG\n","    # print(\"path:\", path)\n","    combined_csi_data=np.asarray([[[[\"-1\"]]]])\n","\n","    for folder in sorted(os.listdir(path)):\n","        one_gesture_data = np.asarray([[[\"-1\"]]])\n","\n","        for file in sorted(os.listdir(os.path.join(path, folder))):\n","            filepath = os.path.join(path, folder, file)\n","            csireader = csiread.Nexmon(filepath, chip='4339', bw=20)\n","            csireader.read()\n","            one_sample = csireader.csi\n","\n","            ### PREPROCESSING ADJUSTMENTS ###\n","            bad_packets = [0, 1, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n","            one_sample = clean(one_sample, log = False, drop = bad_packets, remove_bg = (version in (4, 5)), bg_remove_first=(version == 5), trim = trimmed, filter = 'gaussian', filter_size = 3, window_size = 20) # <-- CHANGE THIS TO ADJUST PREPROCESSING\n","            #################################\n","\n","            # generate graphs if specified\n","            # PLOTS ARE SAVED IN THE \"plots\" FOLDER IN /REU\n","            if create_graphs:\n","                plot_name = re.sub('.pcap', '', file)\n","                dataset_name = os.path.basename(os.path.normpath(path))\n","                plot_location = os.path.join('REU','plots',f'{dataset_name}_plots', folder)\n","                os.makedirs(plot_location, exist_ok = True)\n","                subcarSpectrogram(one_sample, save_path = os.path.join(plot_location, f'{plot_name}_spectrogram'))\n","                subcarLineChart(one_sample, save_path = os.path.join(plot_location, f'{plot_name}_line'))\n","\n","            if one_gesture_data[0][0][0] == \"-1\":\n","                one_gesture_data = np.asarray([np.asarray(one_sample)])\n","            else:\n","                one_gesture_data = np.append(one_gesture_data, [one_sample], axis=0)\n","\n","        gesture_folder = os.path.join(path, folder)\n","        if combined_csi_data[0][0][0][0] == \"-1\":\n","            print(combined_csi_data.shape, one_gesture_data.shape, \"Number of Samples:\", len(os.listdir(gesture_folder)), gesture_folder)\n","            combined_csi_data = np.asarray([np.copy(one_gesture_data)])\n","        else:\n","            print(combined_csi_data.shape, one_gesture_data.shape, \"Number of Samples:\", len(os.listdir(gesture_folder)), gesture_folder)\n","            combined_csi_data = np.append(combined_csi_data, [one_gesture_data], axis=0)\n","    return combined_csi_data\n","\n","def read_csi(dataset_folder_path, dataset_numpy_path, trimmed = False, version = 3, use_cache = True, create_graphs = False):\n","    # check if cache exists and will be used\n","    if use_cache and os.path.exists(dataset_numpy_path): # original\n","        return np.load(dataset_numpy_path)\n","    else:\n","        return process_dataset_folder(dataset_folder_path, trimmed = trimmed, version = version, create_graphs = create_graphs)\n","\n","# this code has non-functional parts that I didn't bother to change\n","def preprocess(dataset_nums, version = 3, trimmed = False, create_graphs = False):\n","\n","    root_data_path = os.path.join('REU','csi_data_all')\n","\n","    # had to patch earlier code\n","    dataset_nums = tuple([int(dataset_nums)])\n","\n","    # save_folder = \"csi_numpy_v4\" if trimmed else \"csi_numpy_v4_unchopped\"\n","    save_folder = 'csi_numpy'\n","    if version == 4: save_folder += \"_v4\"\n","    if version == 5: save_folder += '_v5'\n","    if not trimmed: save_folder += '_unchopped'\n","\n","    os.makedirs(os.path.join(root_data_path, save_folder), exist_ok=True)\n","\n","    # print(root_data_path + dataset_nums[0] + \"/\")\n","    print(f\"processing dataset {dataset_nums[0]}\")\n","    # dataset_numpy_path = f'{root_data_path}csi_numpy_v4/csi_data_{dataset_nums[0]}.npy'\n","    dataset_numpy_path = os.path.join(root_data_path, save_folder, f'csi_data_{dataset_nums[0]}.npy')\n","    print('saving in...', dataset_numpy_path)\n","    # dataset_folder_path = f\"{root_data_path}csi_data_{dataset_nums[0]}/\"\n","    dataset_folder_path = os.path.join(root_data_path, f'csi_data_{dataset_nums[0]}')\n","    combined_csi_data = read_csi(dataset_folder_path, dataset_numpy_path, trimmed = trimmed, version = version, use_cache = False, create_graphs = create_graphs)\n","    np.save(dataset_numpy_path, combined_csi_data)\n","    return combined_csi_data\n","\n","### loader function ###\n","# put this at the top of your model and you should be set!\n","# don't forget to call it\n","\n","# load data\n","def npy_files_to_numpy(dataset_nums, preprocessing_version, from_chopped = True):\n","    datasets = []\n","    csi_numpy_folder = ''\n","    if preprocessing_version == 3:\n","        csi_numpy_folder = 'csi_numpy'\n","    elif preprocessing_version == 4:\n","        csi_numpy_folder = 'csi_numpy_v4'\n","    elif preprocessing_version == 5:\n","        csi_numpy_folder = 'csi_numpy_v5'\n","    else:\n","        raise Exception('Invalid preprocessing_version!!')\n","\n","    for dataset_num in dataset_nums:\n","        dataset_path = ''\n","        if from_chopped:\n","            dataset_path = os.path.join(root_data_path, csi_numpy_folder, f'csi_data_{dataset_num}.npy')\n","        else :\n","            dataset_path = os.path.join(root_data_path, f'{csi_numpy_folder}_unchopped', f'csi_data_{dataset_num}.npy')\n","\n","        # check if they need preprocessing\n","        if not os.path.isfile(dataset_path):\n","            preprocess(dataset_num, version = preprocessing_version, trimmed = from_chopped)\n","\n","        dataset = np.load(dataset_path)\n","        datasets.append(dataset)\n","        print(\"imported\", dataset_path, dataset.shape)\n","\n","    merged_datasets = np.concatenate(datasets, axis = 1)\n","    # classes = os.listdir(f'{root_data_path}csi_data_{dataset_num}')\n","    classes = sorted(os.listdir(os.path.join(root_data_path, f'csi_data_{dataset_num}')))\n","    print(f'dataset {dataset_num} classes: ', classes)\n","\n","    # return the data and class names\n","    return (merged_datasets, np.asarray(classes))\n","\n","def single_dataset_from_flattened_numpy(samples, labels_numeric, batch_size, random_seed = int(np.random.rand() * 100)):\n","    # LABELS SHOULD HAVE THE SAME LENGTH AS FLATTENED DATA!!!\n","    rng = np.random.RandomState(random_seed) # passing the shuffle seed\n","    samples = rng.permutation(samples)\n","    rng = np.random.RandomState(random_seed) # reset the shuffle seed\n","    labels_numeric = rng.permutation(labels_numeric)\n","\n","    return tf.data.Dataset.from_tensor_slices((samples, labels_numeric)).batch(batch_size)\n","\n","def single_dataset_from_non_flat_numpy(merged_data, classes, batch_size, random_seed = int(np.random.rand() * 100), give_numpy = False): # only used for cross validation\n","    labels = []\n","    print(classes)\n","    print(merged_data.shape)\n","    for gesture_index in range(len(merged_data)):\n","        for sample_index in range(len(merged_data[gesture_index])):\n","            labels.append(classes[gesture_index])\n","\n","    # flatten gesture and sample dimensions\n","    old_shape = merged_data.shape\n","    samples = merged_data.reshape(-1, old_shape[2], old_shape[3])\n","    print(\"number of samples:\", len(samples))\n","    labels = np.asarray(labels)\n","\n","    # print('labels before randomization:', labels) # DEBUG\n","\n","    # normalize all input data so that they are between 0 & 1\n","    # INPUT NORMALIZATION HANDLED AT THE MODEL\n","    # samples = np.asarray([samples[i] / np.max(samples[i]) for i in range(len(samples))])\n","\n","    # shuffle data and labels with same seed\n","    num_labels = len(classes)\n","    label_to_index = dict(zip(classes, range(num_labels)))\n","    labels = np.array([label_to_index[label] for label in labels])\n","\n","    rng = np.random.RandomState(random_seed) # passing the shuffle seed\n","    samples = rng.permutation(samples)\n","    rng = np.random.RandomState(random_seed) # reset the shuffle seed\n","    labels = rng.permutation(labels)\n","\n","    if give_numpy:\n","        return (samples, labels)\n","\n","    return tf.data.Dataset.from_tensor_slices((samples, labels)).batch(batch_size)\n","\n","def create_datasets(merged_data, classes, train_ratio, valid_ratio, batch_size, random_seed = int(np.random.rand() * 100)):\n","    labels = []\n","    for gesture_index in range(len(merged_data)):\n","        for sample_index in range(len(merged_data[gesture_index])):\n","            labels.append(classes[gesture_index])\n","\n","    # flatten gesture and sample dimensions\n","    old_shape = merged_data.shape\n","    samples = merged_data.reshape(-1, old_shape[2], old_shape[3])\n","    print(\"total number of samples:\", len(samples))\n","    labels = np.asarray(labels)\n","\n","    # print('labels before randomization:', labels) # DEBUG\n","\n","    # normalize all input data so that they are between 0 & 1\n","    # INPUT NORMALIZATION HANDLED AT THE MODEL\n","    # samples = np.asarray([samples[i] / np.max(samples[i]) for i in range(len(samples))])\n","\n","    # shuffle data and labels with same seed\n","    num_labels = len(classes)\n","    label_to_index = dict(zip(classes, range(num_labels)))\n","    labels = np.array([label_to_index[label] for label in labels])\n","\n","    rng = np.random.RandomState(random_seed) # passing the shuffle seed\n","    samples = rng.permutation(samples)\n","    rng = np.random.RandomState(random_seed) # reset the shuffle seed\n","    labels = rng.permutation(labels)\n","\n","    ds = tf.data.Dataset.from_tensor_slices((samples, labels))\n","\n","    len_ds = len(ds)\n","    num_train = int(len(ds) * train_ratio)\n","    print('LEN OF DS:', len(ds))\n","    num_valid = int(len(ds) * valid_ratio)\n","    num_test = len_ds - num_train - num_valid # test size is implicit\n","\n","    train_ds = ds.take(num_train).batch(batch_size) # from the beginning, take num_train amount\n","    valid_ds = ds.skip(num_train).take(num_valid).batch(batch_size) # skip num train, take num_valid amount\n","    test_ds = ds.skip(num_train).skip(num_valid).batch(batch_size) # skip num train, skip num_valid, take the remaining\n","\n","    print(\"total samples:\", len(ds))\n","    print(\"train_batches:\", len(train_ds), 'samples:', num_train)\n","    print('valid_batches:', len(valid_ds), 'samples:', num_valid)\n","    print(\"test_baches:\", len(test_ds), 'samples:', num_test)\n","\n","    return train_ds, valid_ds, test_ds\n","\n","def get_labels(dataset):\n","    return np.concatenate([y for x, y in dataset], axis = 0)\n","\n","# functions for training / testing model\n","\n","def plot_confusion_matrix(conf_matrix, labels):\n","    conf_matrix = np.round(conf_matrix, 2) # round to 2 decimal places\n","    df_cm = pd.DataFrame(conf_matrix, index = labels, columns = labels)\n","    sn.heatmap(df_cm, annot=True, cmap = 'mako')\n","\n","def confusion_matrix (dataset, model, classes, include_precision_recall = False, plot = True): # remember to pass you test dataset\n","    predictions = np.array([np.argmax(predlist) for predlist in model.predict(dataset)])\n","    # print(predictions)\n","    num_classes = len(classes)\n","    conf_matrix = np.asarray(tf.math.confusion_matrix(get_labels(dataset), predictions, num_classes = num_classes))\n","    precisions = np.asarray([conf_matrix[i][i] / np.sum(conf_matrix[:, i]) for i in range(len(conf_matrix))])\n","    recalls = np.asarray([conf_matrix[i][i] / np.sum(conf_matrix[i, :]) for i in range(len(conf_matrix))])\n","\n","    # note: I'm adding 1e-7 to the sum of the row to avoid nan when dividing by 0 in an empty row\n","    conf_matrix_normalized = np.asarray([conf_matrix[idx] / (np.sum(row) + 1e-7) for idx, row in enumerate(conf_matrix)]) # normalize matrix\n","    if plot:\n","        plot_confusion_matrix(conf_matrix_normalized, classes)\n","\n","    return (conf_matrix_normalized, precisions, recalls) if include_precision_recall else (conf_matrix_normalized,) # in case we need to access the matrix values\n","\n","def train_and_evaluate(model, train_ds, test_ds, classes, epochs, valid_ds = None, lr_scheduler = None, silent = False, include_precision_recall = False):\n","    display(model.summary())\n","\n","    history = model.fit(\n","        train_ds,\n","        validation_data = valid_ds, # if nothing passed, this is ignored\n","        epochs = epochs,\n","        verbose = 1,\n","        callbacks = [keras.callbacks.LearningRateScheduler(lr_scheduler)] if lr_scheduler else None\n","    )\n","\n","    # print(history.history.keys())\n","    if not silent:\n","        plt.plot(history.history['acc'])\n","        if valid_ds is not None:\n","            plt.plot(history.history['val_acc'])\n","        plt.title('Accuracy')\n","        plt.xlabel('epoch')\n","        plt.xlabel('accuracy')\n","        plt.legend(['train','val'], loc='upper left')\n","        plt.grid()\n","        plt.show()\n","\n","        plt.plot(history.history['loss'])\n","        if valid_ds is not None:\n","            plt.plot(history.history['val_loss'])\n","        plt.title('Loss')\n","        plt.xlabel('epoch')\n","        plt.xlabel('loss')\n","        plt.legend(['train','val'], loc='upper left')\n","        plt.grid()\n","        plt.show()\n","\n","    print(\"Evaluation:\")\n","    loss, accuracy = model.evaluate(test_ds)\n","\n","    response = confusion_matrix(test_ds, model, classes, plot = not silent, include_precision_recall = include_precision_recall)\n","\n","    return (accuracy, *response)\n","\n","def build_model(classes, init_learning_rate, complexity_scale = None, dense_regularizer_intensity = None, conv_regularizer_intensity = None):\n","    custom_model = models.Sequential([\n","        layers.BatchNormalization(input_shape = (20, 50, 1)),\n","        layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.Conv2D(16, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.MaxPooling2D(2, strides=2),\n","\n","        layers.BatchNormalization(),\n","        layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.MaxPooling2D(2, strides=2),\n","\n","        layers.BatchNormalization(),\n","        layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_regularizer = regularizers.l2(conv_regularizer_intensity)),\n","        layers.MaxPooling2D(2, strides=2),\n","\n","        layers.Flatten(),\n","\n","        layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(dense_regularizer_intensity)),\n","        layers.Dropout(0.5),\n","        layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(dense_regularizer_intensity)),\n","        layers.Dropout(0.5),\n","        layers.Dense(len(classes), activation = 'softmax')\n","    ])\n","\n","    custom_model.compile(\n","        loss = keras.losses.SparseCategoricalCrossentropy(),\n","        metrics = ['acc'],\n","        optimizer = keras.optimizers.Adam(learning_rate = init_learning_rate),\n","    )\n","\n","    return custom_model\n","\n","def create_and_run(dataset_nums_full_train, preprocessing_version, trimmed, batch_size, epochs, complexity_scale, dense_regularizer_intensity, conv_regularizer_intensity,\n","                   dataset_nums_test = None, plotting = False, init_learning_rate = 0.001, lr_scheduler = None, cross_validation_num = None,\n","                #    hide_partial_train = True\n","                   ):\n","\n","    # if hide_partial_train is False:\n","    #     raise Exception('hide_partial_train is broken. leave it be...')\n","    # # in case someone forgets to pass an explicit partial_train before setting \"hide_partial_train\"\n","    # if hide_partial_train and dataset_nums_test is None :\n","    #     raise Exception(\"Can't hide partial train if it's not provided!\")\n","\n","    data, classes = npy_files_to_numpy(dataset_nums_full_train, preprocessing_version, from_chopped = trimmed) # no background, chopped\n","\n","    print('merged data shape:', data.shape)\n","    subcarSpectrogram(data[3, 21]) # see a sample of the input data\n","\n","    # data_explicit_test is only set when we pass something to \"dataset_nums_test\"\n","    data_explicit_test, _ = npy_files_to_numpy(dataset_nums_test, preprocessing_version, from_chopped = trimmed) if dataset_nums_test is not None else (None, None)\n","\n","    if cross_validation_num is not None:\n","        if dataset_nums_test is not None: # prevent using explicit test data with cross-validation\n","            raise Exception(\"Can't use cross validation with explicit test datasets!!\")\n","\n","        kfold = sklearn.model_selection.KFold(n_splits=cross_validation_num)\n","        data, labels = single_dataset_from_non_flat_numpy(data, classes, batch_size, give_numpy = True) # this does the shuffling\n","\n","        # Initialize lists to store evaluation results\n","        accuracy_scores = []\n","        confusion_matrices = []\n","        all_precisions = []\n","        all_recalls = []\n","\n","        for fold, (train_index, test_index) in enumerate(kfold.split(data)):\n","            print(f\"Fold: {fold+1}\")\n","\n","            # print('train_index', train_index)\n","            # print('test_index', test_index)\n","\n","            # Split the data into train and test sets\n","            train_data = data[train_index]\n","            train_labels = labels[train_index]\n","            test_data = data[test_index]\n","            test_labels = labels[test_index]\n","\n","            # create tf datasets\n","            train_ds = single_dataset_from_flattened_numpy(train_data, train_labels, batch_size)\n","            test_ds = single_dataset_from_flattened_numpy(test_data, test_labels, batch_size)\n","\n","            # if I put this outside it will remember its weights\n","            custom_model = build_model(classes, init_learning_rate, complexity_scale, dense_regularizer_intensity = dense_regularizer_intensity, conv_regularizer_intensity = conv_regularizer_intensity)\n","            # accuracy, conf_matrix = train_and_evaluate(custom_model, train_ds, test_ds, classes, epochs, lr_scheduler=lr_scheduler, silent = True)\n","            accuracy, conf_matrix, precisions, recalls = train_and_evaluate(custom_model, train_ds, test_ds, classes, epochs, lr_scheduler=lr_scheduler, silent = True, include_precision_recall = True)\n","\n","            print('accuracy: ', accuracy)\n","            print('precisions: ', precisions)\n","            print('recalls: ', recalls)\n","            print(f'conf matrix of fold {fold+1}: \\n', conf_matrix)\n","\n","            accuracy_scores.append(accuracy)\n","            all_precisions.append(precisions)\n","            all_recalls.append(recalls)\n","            confusion_matrices.append(conf_matrix)\n","\n","        average_acc = np.mean(np.asarray(accuracy_scores))\n","        average_precisions = np.mean(np.asarray(all_precisions), axis = 0)\n","        average_recalls = np.mean(np.asarray(all_recalls), axis = 0)\n","        average_conf_matrix = np.mean(np.asarray(confusion_matrices), axis = 0)\n","\n","        print('COPY AFTER HERE -----------------------------')\n","        print('average acc: ', average_acc)\n","        print('average precisions: ', average_precisions)\n","        print('average recalls: ', average_recalls)\n","        print('average conf matrix \\n', average_conf_matrix)\n","        plot_confusion_matrix(average_conf_matrix, classes)\n","\n","        return average_acc\n","\n","    else: # if no cross validation, use validation dataset, and plot the accuracy + loss afterwards\n","        train_ratio = 0.8\n","        valid_ratio = 0.1\n","        custom_model = build_model(classes, init_learning_rate, complexity_scale, dense_regularizer_intensity = dense_regularizer_intensity, conv_regularizer_intensity = conv_regularizer_intensity)\n","\n","        if data_explicit_test is not None:\n","            print('using selected test datasets as test data...')\n","            train_ds = single_dataset_from_non_flat_numpy(data, classes, batch_size) # CONFIRMED\n","            test_ds = single_dataset_from_non_flat_numpy(data_explicit_test, classes, batch_size) # CONFIRMED\n","            # if hide_partial_train:\n","            # else:\n","            #     print('using subset of partial train as test data...')\n","            #     train_ds = single_dataset_from_non_flat_numpy(data, classes, batch_size)\n","            #     more_train_and_test_ds = single_dataset_from_non_flat_numpy(data_explicit_test, classes, batch_size)\n","            #     more_train_ds, test_ds = sklearn.model_selection.train_test_split(more_train_and_test_ds, 0.4)\n","            #     train_ds = train_ds.concatenate(more_train_ds) # add some of partial train to train\n","            #     # both train_ds and test_ds should be set\n","\n","            accuracy, conf_matrix = train_and_evaluate(custom_model, train_ds, test_ds, classes, epochs, lr_scheduler = lr_scheduler) # problem\n","        else:\n","            train_ds, valid_ds, test_ds = create_datasets(data, classes, train_ratio, valid_ratio, batch_size) # CONFIRMED\n","            accuracy, conf_matrix, precisions, recalls = train_and_evaluate(custom_model, train_ds, test_ds, classes, epochs, valid_ds = valid_ds, lr_scheduler = lr_scheduler, include_precision_recall = True)\n","\n","def create_step_decay(drop = 0.8, update_interval = 10):\n","    def step_decay(epoch_num, lr):\n","        return lr * drop if epoch_num % update_interval == 0 else lr\n","\n","    return step_decay\n","\n","def create_exponential_decay(drop = 0.99, update_interval = 10):\n","    def exponential_decay(epoch_num, lr):\n","        return drop ** epoch_num * lr if epoch_num % update_interval == 0 else lr\n","\n","    return exponential_decay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9V2WQ9nl9tw9"},"outputs":[],"source":["# impact_factors = {\n","#     'overall': {\n","#         '1': tuple(range(41, 53)),\n","#     },\n","#     'phones': {\n","#         '1': (47, 48), # white\n","#         '2': (49, 50), # black\n","#     },\n","#     'people': {\n","#         '1': (51, 52), # p1\n","#         '2': (60, 61), # p2\n","#         '3': (62, 63), # p3\n","#         '4': (64, 65), # p4\n","#         'combined': (51, 52, 60, 61, 62, 63, 64, 65),\n","#     },\n","#     'dist-AP': {\n","#         \"60''\": (45, 46),\n","#         \"20''\": (51, 52),\n","#     },\n","#     'dist-phone': {\n","#         \"60''\": (47, 48),\n","#         \"20''\": (51, 52),\n","#     },\n","#     'data-points': { # white, setup 3, p1\n","#         '1': (68,),\n","#         '2': (68, 69),\n","#         '3': (68, 69, 70),\n","#         '4': (68, 69, 70, 71),\n","#     }\n","# }\n","\n","create_and_run(\n","    dataset_nums_full_train = tuple(range(41, 53)),\n","    # dataset_nums_test = (65,), \n","    # hide_partial_train = False, # BROKEN\n","    preprocessing_version = 4,\n","    trimmed = True, # if you choose false be sure to change your input shape\n","    batch_size = 64,\n","    epochs = 150,\n","    complexity_scale = 3,\n","    dense_regularizer_intensity = 0.1, \n","    conv_regularizer_intensity = 0, \n","    init_learning_rate = 0.0005,\n","    # lr_scheduler = create_step_decay(drop = 0.8, update_interval = 10),\n","    cross_validation_num = 5,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MzAHGFiG64CQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
